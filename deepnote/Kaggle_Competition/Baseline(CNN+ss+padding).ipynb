{
  "cells": [
    {
      "cell_type": "code",
      "source": "# import the libraries\nimport numpy as np\nimport dask.dataframe as dd\nfrom dask.diagnostics import ProgressBar\nfrom itertools import product\nimport matplotlib.pyplot as plt\n# from fitter import Fitter, get_common_distributions, get_distributions",
      "metadata": {
        "cell_id": "b88a1db7bffa49d881744a9811dc3f5f",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "b88a1db7bffa49d881744a9811dc3f5f"
    },
    {
      "cell_type": "code",
      "source": "# import, shuffle, and see the data\nddf = dd.read_csv('/kaggle/input/stanford-ribonanza-rna-folding/train_data_p_unp.csv')\nshfl_ddf = ddf.sample(frac = 1, random_state = 42)\nshfl_ddf.head()",
      "metadata": {
        "cell_id": "7e6310daeb894197940a68b51f438755",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "22fdff0037de446eb328e2ded97a5869"
    },
    {
      "cell_type": "code",
      "source": "dms_ddf = ddf.loc[ddf['experiment_type'] == \"DMS_MaP\"]\ntwoa3_ddf = ddf.loc[ddf['experiment_type'] == \"2A3_MaP\"]\ndms_ddf.head()",
      "metadata": {
        "cell_id": "e9daa34480ec4af9a03bb0f946c466da",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "34f0ce5c0e5a493eaf971eeb4e199745"
    },
    {
      "cell_type": "code",
      "source": "# Get the shape of the Dask DataFrame\nshape = dms_ddf.shape\n\n# Extract the number of rows and columns from the shape tuple\nnum_rows, num_columns = shape[0].compute(), shape[1]\n\n# Print the size of the Dask DataFrame\nprint(f\"Number of Rows: {num_rows}\")\nprint(f\"Number of Columns: {num_columns}\")",
      "metadata": {
        "cell_id": "54a79ae4c6524332a956fbe3bff3baaf",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "9a15fac39aa64c6ea96b2b9e92fcfa1e"
    },
    {
      "cell_type": "code",
      "source": "# Modified version to account for probability of unpaired bases from secondary structure predictor. WITH padding.\nbases={'A':0, 'C':1, 'G':2, 'U':3 }\n\ndef one_hot(string, p_unpaired):\n\n    res = np.zeros((5, 457), # Now there are 5 rows in the input vector. 206 pairs vs 457 (kaggle test)\n                   dtype=np.float32)\n    res[4, :] = 1\n\n    for j in range(len(string)):\n        if string[j] in bases: # bases can be 'N' signifying missing: this corresponds to all 0 in the encoding\n            res[ bases[ string[j] ], j ]= 1.\n        res[4, j] = p_unpaired[j]\n\n    return res",
      "metadata": {
        "cell_id": "1064a291de7244d5a6272fd6ed690f5b",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "ff559260b605496493dbcf4c2c85fe86"
    },
    {
      "cell_type": "code",
      "source": "# New padding function for inputs 206 -> 457 input CNN\n\ndef padding(react):\n  react_array = np.zeros(457, # 206 pairs vs 457 (kaggle test), pad everything else with zeros\n            dtype=np.float32)\n\n  react_array[0:len(react)] = react # Insert the known reactivity values (even if NaN). Everything else will be 0.\n\n  return react_array",
      "metadata": {
        "cell_id": "db6babfd866647f98396515d66cbe5cc",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "20ac35b8a52d4a29a41e02687832c284"
    },
    {
      "cell_type": "code",
      "source": "# For p_unpaired, with padding\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BedPeaksDataset(torch.utils.data.IterableDataset):\n\n    def __init__(self, seq, p_unpaired, reactivities):\n        super(BedPeaksDataset, self).__init__()\n        self.seq = seq\n        self.reactivities = reactivities\n        self.p_unpaired = p_unpaired\n\n    def __iter__(self):\n        for i in range(len(self.seq)):\n            yield(one_hot(self.seq[i], self.p_unpaired[i]), padding(self.reactivities[i])) # positive example\n\n# train_dataset = BedPeaksDataset(seqs, reactivities)\n# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers = 0)",
      "metadata": {
        "cell_id": "03f2470d62d34bb8b763c0b296ad1377",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "f6ee8ca2c3754712842eeb03e4314e78"
    },
    {
      "cell_type": "code",
      "source": "class CNN_1d(nn.Module):\n\n    def __init__(self,\n                 n_output_channels = 457,# Normally 206, vs 457\n                 filter_widths = [21, 15, 11, 7],\n                 num_chunks = 5,\n                 #max_pool_factor = 2,\n                 nchannels = [5, 32, 64, 128, 256],\n                 n_hidden = 500,\n                 dropout = 0.2):\n\n        super(CNN_1d, self).__init__()\n        self.rf = 0 # running estimate of the receptive field\n        self.chunk_size = 1 # running estimate of num basepairs corresponding to one position after convolutions\n\n        conv_layers = []\n        for i in range(len(nchannels)-1):\n            conv_layers += [ nn.Conv1d(nchannels[i], nchannels[i+1], filter_widths[i], padding = 0, stride = 2),\n                        nn.BatchNorm1d(nchannels[i+1]), # tends to help give faster convergence: https://arxiv.org/abs/1502.03167\n                        nn.Dropout2d(dropout), # popular form of regularization: https://jmlr.org/papers/v15/srivastava14a.html\n                        #nn.MaxPool1d(max_pool_factor),\n                        nn.ELU(inplace=True)  ] # popular alternative to ReLU: https://arxiv.org/abs/1511.07289\n            assert(filter_widths[i] % 2 == 1) # assume this\n            self.rf += (filter_widths[i] - 1) * self.chunk_size\n            #self.chunk_size *= max_pool_factor\n        # If you have a model with lots of layers, you can create a list first and\n        # then use the * operator to expand the list into positional arguments, like this:\n        self.conv_net = nn.Sequential(*conv_layers)\n\n        # Calculate the output size after convolutions and pooling\n        total_length = 206  # Assuming the width of the input matrix is 206\n        for filter_width in filter_widths:\n            total_length = (total_length - filter_width + 1) # // max_pool_factor\n\n        # Calculate the correct number of features to pass to the first linear layer\n        conv_output_features = nchannels[-1] * total_length\n        override = 1280\n        self.dense_net = nn.Sequential(nn.Linear(override, n_hidden), # override = conv_output_features\n                                        nn.BatchNorm1d(n_hidden),\n                                        nn.Dropout(dropout),\n                                        nn.ELU(inplace=True),\n                                        nn.Linear(n_hidden, n_output_channels))\n\n#         self.conv_net = nn.Sequential(*conv_layers)\n\n#         self.seq_len = num_chunks * self.chunk_size + self.rf # amount of sequence context required\n\n#         print(\"Receptive field:\", self.rf, \"Chunk size:\", self.chunk_size, \"Number chunks:\", num_chunks)\n\n#         self.dense_net = nn.Sequential( nn.Linear(nchannels[-1] * num_chunks, n_hidden),\n#                                         nn.Dropout(dropout),\n#                                         nn.ELU(inplace=True),\n#                                         nn.Linear(n_hidden, n_output_channels) )\n\n    def forward(self, x):\n        net = self.conv_net(x)\n        net = net.view(net.size(0), -1)\n        net = self.dense_net(net)\n        return(net)",
      "metadata": {
        "cell_id": "8c60b05eae1c4477a637378eb72bcdb0",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "106f778f9bf348adaedfe796d8abe74c"
    },
    {
      "cell_type": "code",
      "source": "CNN_1d()",
      "metadata": {
        "cell_id": "b8fd608cb5e84a0fb9173f72318922a1",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "90619a0b6d1a4da7a50ab24860dc1836"
    },
    {
      "cell_type": "code",
      "source": "def run_one_epoch(train_flag, dataloader, cnn_1d, optimizer, device=\"cuda\"):\n\n    torch.set_grad_enabled(train_flag)\n    cnn_1d.train() if train_flag else cnn_1d.eval()\n\n    losses = []\n    accuracies = []\n\n    for (x,y) in dataloader: # collection of tuples with iterator\n        x = x.float()\n        y = y.float()\n        (x, y) = ( x.to(device), y.to(device) ) # transfer data to GPU\n\n        output = cnn_1d(x) # forward pass\n        output = output.squeeze() # remove spurious channel dimension\n        loss = F.mse_loss(output, y).float()\n\n        if train_flag:\n            loss.backward() # back propagation\n            optimizer.step()\n            optimizer.zero_grad()\n\n        losses.append(loss.detach().cpu().numpy())\n        # accuracy = torch.mean( ( (output > 0.0) == (y > 0.5) ).float() ) # output is in logit space so threshold is 0.\n        # accuracies.append(accuracy.detach().cpu().numpy())\n\n    return( np.mean(losses))",
      "metadata": {
        "cell_id": "5fcbffcc45c74d1cb2d36c26b63045c0",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "b82c0c459d554bbd8568de87c4e5c608"
    },
    {
      "cell_type": "code",
      "source": "def train_model(cnn_1d, train_data, validation_data, epochs=100, patience=10, verbose = True): # lr = 0.001, weight_decay = 0\n    \"\"\"\n    Train a 1D CNN model and record accuracy metrics.\n    \"\"\"\n    # Move the model to the GPU here to make it runs there, and set \"device\" as above\n    # TODO CODE\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    cnn_1d.to(device)\n\n    # 1. Make new BedPeakDataset and DataLoader objects for both training and validation data.\n    # TODO CODE\n    #train_dataset = BedPeaksDataset(train_data, cnn_1d.seq_len)\n    #train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers = 0)\n    #validation_dataset = BedPeaksDataset(validation_data, cnn_1d.seq_len)\n    #validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000)\n\n    # 2. Instantiates an optimizer for the model.\n    # TODO CODE\n    optimizer = torch.optim.Adam(cnn_1d.parameters(), amsgrad=True) # lr = lr weight_decay = weight_decay\n\n    # 3. Run the training loop with early stopping.\n    # TODO CODE\n    train_losses = []\n    validation_losses = []\n    patience_counter = patience\n    best_test_loss = np.inf\n    check_point_filename = 'cnn_1d_checkpoint.pt' # to save the best model fit to date\n    for epoch in range(epochs):\n        start_time = timeit.default_timer()\n        train_loss = run_one_epoch(True, train_dataloader, cnn_1d, optimizer, device)\n        validation_loss = run_one_epoch(False, validation_dataloader, cnn_1d, optimizer, device)\n        train_losses.append(train_loss)\n        validation_losses.append(validation_loss)\n        # train_accs.append(train_acc)\n        # val_accs.append(val_acc)\n        if validation_loss < best_test_loss:\n            torch.save(cnn_1d.state_dict(), check_point_filename)\n            best_test_loss = validation_loss\n            patience_counter = patience\n        else:\n            patience_counter -= 1\n            if patience_counter <= 0:\n                cnn_1d.load_state_dict(torch.load(check_point_filename)) # recover the best model so far\n                break\n        elapsed = float(timeit.default_timer() - start_time)\n        print(\"Epoch {} took {:.2f}s. Train loss: {:.4f}., Test loss: {:.4f}. Patience: {}\".format(epoch+1, elapsed, train_loss, validation_loss, patience_counter))\n\n    # 4. Return the fitted model (not strictly necessary since this happens \"in place\"), train and validation accuracies.\n    # TODO CODE\n    return(cnn_1d, train_losses, validation_losses)",
      "metadata": {
        "cell_id": "df474bda3b7c4ccaa39560290876be92",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "925d55df5d2044599e97f2478dd85e9c"
    },
    {
      "cell_type": "code",
      "source": "pip install dask_ml",
      "metadata": {
        "cell_id": "8448b35d82d84e14ab5f2ba11e9bba4e",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "ac4318d09cb54492b77e22e57e4b4ca3"
    },
    {
      "cell_type": "code",
      "source": "# apply SN-filter (with p_unpaired calculation), padding does not affect this block\nfrom dask_ml.model_selection import train_test_split\ndf_sn = ddf[ddf[\"SN_filter\"]==1]\n\n# split into 2A3 MaP and DMS MaP datasets\ndf_2A3 = df_sn[df_sn[\"experiment_type\"]==\"2A3_MaP\"]\ndf_DMS = df_sn[df_sn[\"experiment_type\"]==\"DMS_MaP\"]\n\n# split into train and test\nX_2A3_seq = df_2A3[\"sequence\"]\n#X_2A3 = df_2A3[\"sequence\"]\nX_2A3_p_unpaired = df_2A3.loc[:, \"p_unp_1\":\"p_unp_206\"]\n#X_2A3 = dd.concat([X_2A3_seq, X_2A3_p_unpaired])\n#print(X_2A3.shape)\ny_2A3 = df_2A3.loc[:, df_2A3.columns.str.fullmatch(\"reactivity_\\d\\d\\d\\d\")]\nX_2A3_train_seq, X_2A3_test_seq, X_2A3_train_p_unpaired, X_2A3_test_p_unpaired, y_2A3_train, y_2A3_test = train_test_split(X_2A3_seq, X_2A3_p_unpaired, y_2A3, test_size=0.2, shuffle=True, blockwise=True, random_state=42)\nX_2A3_train_seq, X_2A3_validation_seq, X_2A3_train_p_unpaired, X_2A3_validation_p_unpaired, y_2A3_train, y_2A3_validation = train_test_split(X_2A3_train_seq, X_2A3_train_p_unpaired, y_2A3_train, test_size=0.25, shuffle=True, blockwise=True, random_state=42)\n\nX_DMS_seq = df_DMS[\"sequence\"]\n#X_DMS = df_DMS[\"sequence\"]\nX_DMS_p_unpaired = df_DMS.loc[:, \"p_unp_1\":\"p_unp_206\"]\n#X_DMS = dd.concat([X_DMS_seq, X_DMS_p_unpaired])\n#print(X_DMS.shape)\ny_DMS = df_DMS.loc[:, df_DMS.columns.str.fullmatch(\"reactivity_\\d\\d\\d\\d\")]\nX_DMS_train_seq, X_DMS_test_seq, X_DMS_train_p_unpaired, X_DMS_test_p_unpaired, y_DMS_train, y_DMS_test = train_test_split(X_DMS_seq, X_DMS_p_unpaired, y_DMS, test_size=0.2, shuffle=True, blockwise=True, random_state=42)\nX_DMS_train_seq, X_DMS_validation_seq, X_DMS_train_p_unpaired, X_DMS_validation_p_unpaired, y_DMS_train, y_DMS_validation = train_test_split(X_DMS_train_seq, X_DMS_train_p_unpaired, y_DMS_train, test_size=0.25, shuffle=True, blockwise=True, random_state=42)",
      "metadata": {
        "cell_id": "683e54187b444a059656fc0733b556b8",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "6011fd5b99ba43b789c867abc6591b52"
    },
    {
      "cell_type": "code",
      "source": "def df_toArray(ddf1A, ddf1B, ddf2): # for sequence, p_unpaired, and reactivity. Padding does not affect this block\n    subset_columns = []\n    for i in range(457): #206 vs 457\n        subset_columns.append(\"reactivity_0\"+str(i+1).zfill(3))\n\n    # Use .to_dask_array() to convert the subset of the DataFrame to a Dask Array\n    # subset = small_set[subset_columns]\n\n    # Compute the subset of the Dask DataFrame and convert it to a Pandas DataFrame\n    reactivities = ddf2.compute().to_numpy()\n    p_unpaired = ddf1B.compute().to_numpy()\n\n    row_means = np.nanmean(reactivities, axis=1)\n\n    # Iterate over each element and replace NaN with the row mean\n    for i, row in enumerate(reactivities):\n        mask = np.isnan(row)\n        reactivities[i, mask] = row_means[i]\n\n    reactivities\n\n    seqs = ddf1A.compute().tolist()\n\n    return seqs, p_unpaired, reactivities\n\n# df_toArray(X_2A3_train, y_2A3_train)",
      "metadata": {
        "cell_id": "2cd84fdd4bbd456b98dc11efb462c80b",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "a7502a8a9e8f4c88be255a845e65ceab"
    },
    {
      "cell_type": "code",
      "source": "# For p_unpaired 2A3. Padding does not affect this block.\n\nseqs, p_unpaired, reactivities = df_toArray(X_2A3_train_seq, X_2A3_train_p_unpaired, y_2A3_train)\ntrain_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n\nseqs, p_unpaired, reactivities = df_toArray(X_2A3_validation_seq, X_2A3_validation_p_unpaired, y_2A3_validation)\nvalidation_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\nvalidation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000, num_workers = 0)\n\nseqs, p_unpaired, reactivities = df_toArray(X_2A3_test_seq, X_2A3_test_p_unpaired, y_2A3_test)\ntest_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, num_workers = 0)",
      "metadata": {
        "cell_id": "cca524b5db474e7797accd89a041a61f",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "2220782a5ca34309a141bd6ea70c79de"
    },
    {
      "cell_type": "code",
      "source": "# For p_unpaired DMS. Padding does not affect this block.\n\nseqs, p_unpaired, reactivities = df_toArray(X_DMS_train_seq, X_DMS_train_p_unpaired, y_DMS_train)\ntrain_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n\nseqs, p_unpaired, reactivities = df_toArray(X_DMS_validation_seq, X_DMS_validation_p_unpaired, y_DMS_validation)\nvalidation_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\nvalidation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000, num_workers = 0)\n\nseqs, p_unpaired, reactivities = df_toArray(X_DMS_test_seq, X_DMS_test_p_unpaired, y_DMS_test)\ntest_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, num_workers = 0)",
      "metadata": {
        "cell_id": "2c8364d7ba734385b9efe46a9842e241",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "d5c70aff6bba466cac96b2d1a3aef4af"
    },
    {
      "cell_type": "code",
      "source": "import timeit\nmy_cnn1d_1 = CNN_1d()\n# print(my_cnn1d.seq_le\nmy_cnn1d_1 = my_cnn1d_1.float()\nmy_cnn1d_1, train_losses, test_losses = train_model(my_cnn1d_1, train_dataloader, validation_dataloader) # lr = 0.001 weight_decay = 0",
      "metadata": {
        "cell_id": "b22299cae21645a58afe1ab42a7eec6b",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "4f587e8539184d9ba7587fe26a6c9ea6"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fe343e39-d2c0-4296-915d-091d9a42752d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_execution_queue": []
  }
}