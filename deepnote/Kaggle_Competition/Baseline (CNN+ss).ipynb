{
  "cells": [
    {
      "cell_type": "code",
      "source": "# import the libraries\nimport numpy as np\nimport dask.dataframe as dd\nfrom dask.diagnostics import ProgressBar\nfrom itertools import product\nimport matplotlib.pyplot as plt\n# from fitter import Fitter, get_common_distributions, get_distributions",
      "metadata": {
        "cell_id": "8c485c36e99f4b3ba3ef57907c2e2bbf",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "8c485c36e99f4b3ba3ef57907c2e2bbf"
    },
    {
      "cell_type": "code",
      "source": "# import, shuffle, and see the data\nddf = dd.read_csv('/kaggle/input/stanford-ribonanza-rna-folding/train_data_p_unp.csv')\nshfl_ddf = ddf.sample(frac = 1, random_state = 42)\nshfl_ddf.head()",
      "metadata": {
        "cell_id": "dbbefbbf1a1b47e9a57ef101f5923fa2",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "98f496e164f84cde88168d58088f7e75"
    },
    {
      "cell_type": "code",
      "source": "dms_ddf = ddf.loc[ddf['experiment_type'] == \"DMS_MaP\"]\ntwoa3_ddf = ddf.loc[ddf['experiment_type'] == \"2A3_MaP\"]\ndms_ddf.head()",
      "metadata": {
        "cell_id": "eb093f20190f4e198f179bbfe2cdaf7c",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "be3a95d2198d40258d19f37e297bbe3e"
    },
    {
      "cell_type": "code",
      "source": "# Get the shape of the Dask DataFrame\nshape = dms_ddf.shape\n\n# Extract the number of rows and columns from the shape tuple\nnum_rows, num_columns = shape[0].compute(), shape[1]\n\n# Print the size of the Dask DataFrame\nprint(f\"Number of Rows: {num_rows}\")\nprint(f\"Number of Columns: {num_columns}\")",
      "metadata": {
        "cell_id": "8e27e25bfc784ede9d3929884036dd3d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "80f447434a954894bba73795acd890e8"
    },
    {
      "cell_type": "code",
      "source": "# Modified version to account for probability of unpaired bases from secondary structure predictor\nbases={'A':0, 'C':1, 'G':2, 'U':3 }\n\ndef one_hot(string, p_unpaired):\n\n    res = np.zeros((5, 206), # Now there are 5 rows in the input vector, 457 is maximum length\n                   dtype=np.float32)\n    res[4, :] = 1\n\n    for j in range(len(string)):\n        if string[j] in bases: # bases can be 'N' signifying missing: this corresponds to all 0 in the encoding\n            res[ bases[ string[j] ], j ]= 1.\n        res[4, j] = p_unpaired[j]\n\n    return res",
      "metadata": {
        "cell_id": "2c81cf30f9594ff3b8e28f477a926233",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "b8109cf0d84e4d36a7fb03e2734cb865"
    },
    {
      "cell_type": "code",
      "source": "# For p_unpaired data (changed the yield output)\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BedPeaksDataset(torch.utils.data.IterableDataset):\n\n    def __init__(self, seq, p_unpaired, reactivities):\n        super(BedPeaksDataset, self).__init__()\n        self.seq = seq\n        self.reactivities = reactivities\n        self.p_unpaired = p_unpaired\n\n    def __iter__(self):\n        for i in range(len(self.seq)):\n            yield(one_hot(self.seq[i], self.p_unpaired[i]), self.reactivities[i]) # positive example\n\n# train_dataset = BedPeaksDataset(seqs, reactivities)\n# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers = 0)",
      "metadata": {
        "cell_id": "6b10c5db77d34318814d7563d3962a61",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "97a0d064363943ecb531e1cb6c71020e"
    },
    {
      "cell_type": "code",
      "source": "class CNN_1d(nn.Module):\n\n    def __init__(self,\n                 n_output_channels = 206,# Normally 206, vs 457\n                 filter_widths = [21, 15, 11, 7],\n                 num_chunks = 5,\n                 #max_pool_factor = 2,\n                 nchannels = [5, 32, 64, 128, 256],\n                 n_hidden = 500,\n                 dropout = 0.2):\n\n        super(CNN_1d, self).__init__()\n        self.rf = 0 # running estimate of the receptive field\n        self.chunk_size = 1 # running estimate of num basepairs corresponding to one position after convolutions\n\n        conv_layers = []\n        for i in range(len(nchannels)-1):\n            conv_layers += [ nn.Conv1d(nchannels[i], nchannels[i+1], filter_widths[i], padding = 0, stride = 2),\n                        nn.BatchNorm1d(nchannels[i+1]), # tends to help give faster convergence: https://arxiv.org/abs/1502.03167\n                        nn.Dropout2d(dropout), # popular form of regularization: https://jmlr.org/papers/v15/srivastava14a.html\n                        #nn.MaxPool1d(max_pool_factor),\n                        nn.ELU(inplace=True)  ] # popular alternative to ReLU: https://arxiv.org/abs/1511.07289\n            assert(filter_widths[i] % 2 == 1) # assume this\n            self.rf += (filter_widths[i] - 1) * self.chunk_size\n            #self.chunk_size *= max_pool_factor\n        # If you have a model with lots of layers, you can create a list first and\n        # then use the * operator to expand the list into positional arguments, like this:\n        self.conv_net = nn.Sequential(*conv_layers)\n\n        # Calculate the output size after convolutions and pooling\n        total_length = 206  # Assuming the width of the input matrix is 206\n        for filter_width in filter_widths:\n            total_length = (total_length - filter_width + 1) # // max_pool_factor\n\n        # Calculate the correct number of features to pass to the first linear layer\n        conv_output_features = nchannels[-1] * total_length\n        override = 1280\n        self.dense_net = nn.Sequential(nn.Linear(override, n_hidden), # override = conv_output_features\n                                        nn.BatchNorm1d(n_hidden),\n                                        nn.Dropout(dropout),\n                                        nn.ELU(inplace=True),\n                                        nn.Linear(n_hidden, n_output_channels))\n\n#         self.conv_net = nn.Sequential(*conv_layers)\n\n#         self.seq_len = num_chunks * self.chunk_size + self.rf # amount of sequence context required\n\n#         print(\"Receptive field:\", self.rf, \"Chunk size:\", self.chunk_size, \"Number chunks:\", num_chunks)\n\n#         self.dense_net = nn.Sequential( nn.Linear(nchannels[-1] * num_chunks, n_hidden),\n#                                         nn.Dropout(dropout),\n#                                         nn.ELU(inplace=True),\n#                                         nn.Linear(n_hidden, n_output_channels) )\n\n    def forward(self, x):\n        net = self.conv_net(x)\n        net = net.view(net.size(0), -1)\n        net = self.dense_net(net)\n        return(net)",
      "metadata": {
        "cell_id": "781eeff1626a417483b949ebd19bcba6",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "67eb473867784f8faafcc4aa9792bdc6"
    },
    {
      "cell_type": "code",
      "source": "CNN_1d()",
      "metadata": {
        "cell_id": "b1123f5fe435443aae3e8246b0cef2f7",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "ac1e53b2a577445481796db1f9f7f418"
    },
    {
      "cell_type": "code",
      "source": "def run_one_epoch(train_flag, dataloader, cnn_1d, optimizer, device=\"cuda\"):\n\n    torch.set_grad_enabled(train_flag)\n    cnn_1d.train() if train_flag else cnn_1d.eval()\n\n    losses = []\n    accuracies = []\n\n    for (x,y) in dataloader: # collection of tuples with iterator\n        x = x.float()\n        y = y.float()\n        (x, y) = ( x.to(device), y.to(device) ) # transfer data to GPU\n\n        output = cnn_1d(x) # forward pass\n        output = output.squeeze() # remove spurious channel dimension\n        loss = F.mse_loss(output, y).float()\n\n        if train_flag:\n            loss.backward() # back propagation\n            optimizer.step()\n            optimizer.zero_grad()\n\n        losses.append(loss.detach().cpu().numpy())\n        # accuracy = torch.mean( ( (output > 0.0) == (y > 0.5) ).float() ) # output is in logit space so threshold is 0.\n        # accuracies.append(accuracy.detach().cpu().numpy())\n\n    return( np.mean(losses))",
      "metadata": {
        "cell_id": "10f247689799431daabe0694f8ae35cf",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "06e51e8caf29463c92806decfc7940c1"
    },
    {
      "cell_type": "code",
      "source": "def train_model(cnn_1d, train_data, validation_data, epochs=100, patience=10, verbose = True): # lr = 0.001, weight_decay = 0\n    \"\"\"\n    Train a 1D CNN model and record accuracy metrics.\n    \"\"\"\n    # Move the model to the GPU here to make it runs there, and set \"device\" as above\n    # TODO CODE\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    cnn_1d.to(device)\n\n    # 1. Make new BedPeakDataset and DataLoader objects for both training and validation data.\n    # TODO CODE\n    #train_dataset = BedPeaksDataset(train_data, cnn_1d.seq_len)\n    #train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers = 0)\n    #validation_dataset = BedPeaksDataset(validation_data, cnn_1d.seq_len)\n    #validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000)\n\n    # 2. Instantiates an optimizer for the model.\n    # TODO CODE\n    optimizer = torch.optim.Adam(cnn_1d.parameters(), amsgrad=True) # lr = lr weight_decay = weight_decay\n\n    # 3. Run the training loop with early stopping.\n    # TODO CODE\n    train_losses = []\n    validation_losses = []\n    patience_counter = patience\n    best_test_loss = np.inf\n    check_point_filename = 'cnn_1d_checkpoint.pt' # to save the best model fit to date\n    for epoch in range(epochs):\n        start_time = timeit.default_timer()\n        train_loss = run_one_epoch(True, train_dataloader, cnn_1d, optimizer, device)\n        validation_loss = run_one_epoch(False, validation_dataloader, cnn_1d, optimizer, device)\n        train_losses.append(train_loss)\n        validation_losses.append(validation_loss)\n        # train_accs.append(train_acc)\n        # val_accs.append(val_acc)\n        if validation_loss < best_test_loss:\n            torch.save(cnn_1d.state_dict(), check_point_filename)\n            best_test_loss = validation_loss\n            patience_counter = patience\n        else:\n            patience_counter -= 1\n            if patience_counter <= 0:\n                cnn_1d.load_state_dict(torch.load(check_point_filename)) # recover the best model so far\n                break\n        elapsed = float(timeit.default_timer() - start_time)\n        print(\"Epoch {} took {:.2f}s. Train loss: {:.4f}., Test loss: {:.4f}. Patience: {}\".format(epoch+1, elapsed, train_loss, validation_loss, patience))\n\n    # 4. Return the fitted model (not strictly necessary since this happens \"in place\"), train and validation accuracies.\n    # TODO CODE\n    return(cnn_1d, train_losses, validation_losses)",
      "metadata": {
        "cell_id": "005d6d2fefc94faf807584ceff9d7c6d",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "e2b64e9b1c5247cc8bdcdd8f7b5e7bad"
    },
    {
      "cell_type": "code",
      "source": "pip install dask_ml",
      "metadata": {
        "cell_id": "b99994f542f14b3bad6c0bce9dcd8710",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "ca7eaa453523484ba6b7f4717cab7126"
    },
    {
      "cell_type": "code",
      "source": "# apply SN-filter (with p_unpaired calculation)\nfrom dask_ml.model_selection import train_test_split\ndf_sn = ddf[ddf[\"SN_filter\"]==1]\n\n# split into 2A3 MaP and DMS MaP datasets\ndf_2A3 = df_sn[df_sn[\"experiment_type\"]==\"2A3_MaP\"]\ndf_DMS = df_sn[df_sn[\"experiment_type\"]==\"DMS_MaP\"]\n\n# split into train and test\nX_2A3_seq = df_2A3[\"sequence\"]\nX_2A3_p_unpaired = df_2A3.loc[:, \"p_unp_1\":\"p_unp_206\"]\ny_2A3 = df_2A3.loc[:, df_2A3.columns.str.fullmatch(\"reactivity_\\d\\d\\d\\d\")]\nX_2A3_train_seq, X_2A3_test_seq, X_2A3_train_p_unpaired, X_2A3_test_p_unpaired, y_2A3_train, y_2A3_test = train_test_split(X_2A3_seq, X_2A3_p_unpaired, y_2A3, test_size=0.2, shuffle=True, blockwise=True, random_state=42)\nX_2A3_train_seq, X_2A3_validation_seq, X_2A3_train_p_unpaired, X_2A3_validation_p_unpaired, y_2A3_train, y_2A3_validation = train_test_split(X_2A3_train_seq, X_2A3_train_p_unpaired, y_2A3_train, test_size=0.25, shuffle=True, blockwise=True, random_state=42)\n\nX_DMS = df_DMS[\"sequence\"]\ny_DMS = df_DMS.loc[:, df_DMS.columns.str.fullmatch(\"reactivity_\\d\\d\\d\\d\")]\nX_DMS_train, X_DMS_test, y_DMS_train, y_DMS_test = train_test_split(X_DMS, y_DMS, test_size=0.2, shuffle=True, blockwise=True, random_state=42)\nX_DMS_train, X_DMS_validation, y_DMS_train, y_DMS_validation = train_test_split(X_DMS_train, y_DMS_train, test_size=0.25, shuffle=True, blockwise=True, random_state=42)",
      "metadata": {
        "cell_id": "615cdeb0848548dfbc3d5bbb9115f0e3",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "7585285dcec647cfb6d19272eda48f73"
    },
    {
      "cell_type": "code",
      "source": "def df_toArray(ddf1A, ddf1B, ddf2): # for sequence, p_unpaired, and reactivity\n    subset_columns = []\n    for i in range(206):\n        subset_columns.append(\"reactivity_0\"+str(i+1).zfill(3))\n\n    # Use .to_dask_array() to convert the subset of the DataFrame to a Dask Array\n    # subset = small_set[subset_columns]\n\n    # Compute the subset of the Dask DataFrame and convert it to a Pandas DataFrame\n    reactivities = ddf2.compute().to_numpy()\n    p_unpaired = ddf1B.compute().to_numpy()\n\n    row_means = np.nanmean(reactivities, axis=1)\n\n    # Iterate over each element and replace NaN with the row mean\n    for i, row in enumerate(reactivities):\n        mask = np.isnan(row)\n        reactivities[i, mask] = row_means[i]\n\n    reactivities\n\n    seqs = ddf1A.compute().tolist()\n\n    return seqs, p_unpaired, reactivities\n\n# df_toArray(X_2A3_train, y_2A3_train)",
      "metadata": {
        "cell_id": "b3ce4a7f564046419b7a4f15b622fd2c",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "7bd1360ddd9b45268ebecedb312642c3"
    },
    {
      "cell_type": "code",
      "source": "def df_toArray_test(ddf1A, ddf1B, ddf2): # for sequence, p_unpaired, and reactivity\n    subset_columns = []\n    for i in range(206):\n        subset_columns.append(\"reactivity_0\"+str(i+1).zfill(3))\n\n    # Use .to_dask_array() to convert the subset of the DataFrame to a Dask Array\n    # subset = small_set[subset_columns]\n\n    # Compute the subset of the Dask DataFrame and convert it to a Pandas DataFrame\n    reactivities = ddf2.compute().to_numpy()\n    p_unpaired = ddf1B.compute().to_numpy()\n\n    row_means = np.nanmean(reactivities, axis=1)\n\n    # Iterate over each element and replace NaN with the row mean\n    for i, row in enumerate(reactivities):\n        mask = np.isnan(row)\n        reactivities[i, mask] = row_means[i]\n\n    reactivities\n\n    seqs = ddf1A.compute().tolist()\n\n    return seqs, p_unpaired, reactivities\n\n# df_toArray(X_2A3_train, y_2A3_train)",
      "metadata": {
        "cell_id": "fe17b50e8a3b423d88343e9b1548b659",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "eb8eaeb5840741459cae08b2df0c1374"
    },
    {
      "cell_type": "code",
      "source": "# For p_unpaired, 2A3\n\nseqs, p_unpaired, reactivities = df_toArray(X_2A3_train_seq, X_2A3_train_p_unpaired, y_2A3_train)\ntrain_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n\nseqs, p_unpaired, reactivities = df_toArray(X_2A3_validation_seq, X_2A3_validation_p_unpaired, y_2A3_validation)\nvalidation_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\nvalidation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000, num_workers = 0)\n\nseqs, p_unpaired, reactivities = df_toArray(X_2A3_test_seq, X_2A3_test_p_unpaired, y_2A3_test)\ntest_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, num_workers = 0)",
      "metadata": {
        "cell_id": "0240b2972dcf4d9bbd423b9423562781",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "5984cc40149f485c93eeeeee5dfa38a8"
    },
    {
      "cell_type": "code",
      "source": "# For p_unpaired, DMS\n\nseqs, p_unpaired, reactivities = df_toArray(X_DMS_train_seq, X_DMS_train_p_unpaired, y_DMS_train)\ntrain_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n\nseqs, p_unpaired, reactivities = df_toArray(X_DMS_validation_seq, X_DMS_validation_p_unpaired, y_DMS_validation)\nvalidation_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\nvalidation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000, num_workers = 0)\n\nseqs, p_unpaired, reactivities = df_toArray(X_DMS_test_seq, X_DMS_test_p_unpaired, y_DMS_test)\ntest_dataset = BedPeaksDataset(seqs, p_unpaired, reactivities)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, num_workers = 0)",
      "metadata": {
        "cell_id": "edcd65711b4c40758ca1e263bce14c58",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "28f19997c6464447ba258ed3c75ec464"
    },
    {
      "cell_type": "code",
      "source": "import timeit\nmy_cnn1d_1 = CNN_1d()\n# print(my_cnn1d.seq_le\nmy_cnn1d_1 = my_cnn1d_1.float()\nmy_cnn1d_1, train_losses, test_losses = train_model(my_cnn1d_1, train_dataloader, validation_dataloader) # lr = 0.001 weight_decay = 0",
      "metadata": {
        "cell_id": "387fca7b06c74fe7b94af15fd22e7f86",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "2a17b3a1e3444f469d72fc0d71c1284a"
    },
    {
      "cell_type": "code",
      "source": "device = 'cuda'\noutputs = []\nexpected = []\nfor (x,y) in test_dataloader: # iterate over batches\n    # print(y)\n    if torch.cuda.is_available():\n        x = x.to(device)\n    output = my_cnn1d_1(x).squeeze() # your awesome model here!\n    #output = torch.sigmoid(output)\n    output_np = output.detach().cpu().numpy()\n    outputs.append(output_np)\n    expected.append(y.numpy())\noutput_np = np.concatenate(outputs)\nexpected_np = np.concatenate(expected)\n\nprint(output_np)\nprint(expected_np)\n\noutput_np = output_np.clip(0, 1)\nexpected_np = expected_np.clip(0,1)\n\nmae1 = np.mean(np.abs(output_np - expected_np))\nmae1",
      "metadata": {
        "cell_id": "0f7f929c2a4b44d7bf4b9e5a869af9e3",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "a3b3f75111d84ae2890651582e946a61"
    },
    {
      "cell_type": "code",
      "source": "# Save the model\nmodel_name = \"CNN_3_DMS.pth\"\ntorch.save(my_cnn1d_1.state_dict(), path_to_assignment_dir / model_name)",
      "metadata": {
        "cell_id": "c9ec9ad0d5854216ab2ffef777cbf469",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "4282d7c8128a4fa7b51f4484dbc88f86"
    },
    {
      "cell_type": "code",
      "source": "# Plot the results\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (12,8))\nplt.plot(train_losses, label=\"train\")\nplt.plot(test_losses, label=\"validation\")\nplt.title('Training & Validation Loss (DMS) CNN Model 1', fontsize = 24)\nplt.xlabel('Epochs', labelpad = 15, fontsize = 16)\nplt.ylabel('Loss', labelpad = 15, fontsize = 16)\nplt.legend()\nplt.grid(which=\"both\")\nplt.show()",
      "metadata": {
        "cell_id": "8ab333e1d1cc4429834c57486c044ddd",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "07c5f7e8eafa452fbeff9555d860ba18"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "cell_id": "3ef8fda0e42b45e58e68900523f4779f",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "c19ed0ed18cb4a98bea4f0407748ac77"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fe343e39-d2c0-4296-915d-091d9a42752d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {},
    "orig_nbformat": 2,
    "deepnote_execution_queue": []
  }
}