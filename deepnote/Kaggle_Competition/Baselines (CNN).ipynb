{
  "cells": [
    {
      "cell_type": "code",
      "source": "# import the libraries\nimport numpy as np\nimport dask.dataframe as dd\nfrom dask.diagnostics import ProgressBar\nfrom itertools import product\nimport matplotlib.pyplot as plt\n# from fitter import Fitter, get_common_distributions, get_distributions",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:48:45.250063Z",
          "iopub.status.idle": "2023-11-16T21:48:46.635027Z",
          "iopub.execute_input": "2023-11-16T21:48:45.250395Z",
          "shell.execute_reply": "2023-11-16T21:48:46.633829Z",
          "shell.execute_reply.started": "2023-11-16T21:48:45.25036Z"
        },
        "cell_id": "7f1b221f04ff4d1b96e2ca373a6682c2",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "9cea591dd7fa4b16a16d37be2b379a62"
    },
    {
      "cell_type": "code",
      "source": "# import, shuffle, and see the data\nddf = dd.read_csv('/kaggle/input/stanford-ribonanza-rna-folding/train_data.csv')\nshfl_ddf = ddf.sample(frac = 1, random_state = 42)\nshfl_ddf.head()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:48:46.637551Z",
          "iopub.status.idle": "2023-11-16T21:48:49.373642Z",
          "iopub.execute_input": "2023-11-16T21:48:46.638665Z",
          "shell.execute_reply": "2023-11-16T21:48:49.372681Z",
          "shell.execute_reply.started": "2023-11-16T21:48:46.638623Z"
        },
        "cell_id": "cf6777ca6b224d779f1f2b78e749d205",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "aa0c4a73609c4f9bb66fb7f24bee1b08"
    },
    {
      "cell_type": "code",
      "source": "dms_ddf = ddf.loc[ddf['experiment_type'] == \"DMS_MaP\"]\ntwoa3_ddf = ddf.loc[ddf['experiment_type'] == \"2A3_MaP\"]\ndms_ddf.head()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:48:49.37521Z",
          "iopub.status.idle": "2023-11-16T21:48:50.668134Z",
          "iopub.execute_input": "2023-11-16T21:48:49.375807Z",
          "shell.execute_reply": "2023-11-16T21:48:50.667483Z",
          "shell.execute_reply.started": "2023-11-16T21:48:49.37577Z"
        },
        "cell_id": "8feb0cfe21ed4c46851f78eeb0d40110",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "722c94339102439a937244f50cdbf3c9"
    },
    {
      "cell_type": "code",
      "source": "# Get the shape of the Dask DataFrame\nshape = dms_ddf.shape\n\n# Extract the number of rows and columns from the shape tuple\nnum_rows, num_columns = shape[0].compute(), shape[1]\n\n# Print the size of the Dask DataFrame\nprint(f\"Number of Rows: {num_rows}\")\nprint(f\"Number of Columns: {num_columns}\")",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:48:50.670089Z",
          "iopub.status.idle": "2023-11-16T21:49:32.272541Z",
          "iopub.execute_input": "2023-11-16T21:48:50.670566Z",
          "shell.execute_reply": "2023-11-16T21:49:32.27129Z",
          "shell.execute_reply.started": "2023-11-16T21:48:50.670543Z"
        },
        "cell_id": "bd1e18ba0d4d48e9b3c119b115486a01",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "1c2d77f14ab74bfbbf0197c0199ad4b1"
    },
    {
      "cell_type": "code",
      "source": "# small_set = dms_ddf.head(10)\n# small_set.head()\n\n# subset_columns = []\n# for i in range(206):\n#     subset_columns.append(\"reactivity_0\"+str(i+1).zfill(3))\n\n# # Use .to_dask_array() to convert the subset of the DataFrame to a Dask Array\n# # subset = small_set[subset_columns]\n\n# # Compute the subset of the Dask DataFrame and convert it to a Pandas DataFrame\n# reactivities = small_set[subset_columns].to_numpy()\n\n# row_means = np.nanmean(seqs, axis=1)\n\n# # Iterate over each element and replace NaN with the row mean\n# for i, row in enumerate(reactivities):\n#     mask = np.isnan(row)\n#     reactivities[i, mask] = row_means[i]\n\n# reactivities\n\n# seqs = small_set['sequence'].tolist()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:32.274014Z",
          "iopub.status.idle": "2023-11-16T21:49:32.279559Z",
          "iopub.execute_input": "2023-11-16T21:49:32.274464Z",
          "shell.execute_reply": "2023-11-16T21:49:32.278504Z",
          "shell.execute_reply.started": "2023-11-16T21:49:32.274429Z"
        },
        "cell_id": "25c48c87bac74c5ab7b6e43c023abf72",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "0011479a79ed4ba7879ea7558e483ffb"
    },
    {
      "cell_type": "code",
      "source": "bases={'A':0, 'C':1, 'G':2, 'T':3 }\n\ndef one_hot(string):\n\n    res = np.zeros((5, 206),\n                   dtype=np.float32)\n\n    for j in range(len(string)):\n        if string[j] in bases: # bases can be 'N' signifying missing: this corresponds to all 0 in the encoding\n            res[ bases[ string[j] ], j ]= 1.\n    for j in range(len(string),206):\n        res[4, j]= 1.\n    return res\n# one_hot(seqs[0])",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:32.280717Z",
          "iopub.status.idle": "2023-11-16T21:49:32.295538Z",
          "iopub.execute_input": "2023-11-16T21:49:32.281039Z",
          "shell.execute_reply": "2023-11-16T21:49:32.293227Z",
          "shell.execute_reply.started": "2023-11-16T21:49:32.281016Z"
        },
        "cell_id": "1d532acc6eac46a19e89a3b90ab398eb",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "b39bbed33a134e3d94521f4b72e4fc70"
    },
    {
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BedPeaksDataset(torch.utils.data.IterableDataset):\n\n    def __init__(self, seq, reactivities):\n        super(BedPeaksDataset, self).__init__()\n        self.seq = seq\n        self.reactivities = reactivities\n\n    def __iter__(self):\n        for i in range(len(self.seq)):\n            yield(one_hot(self.seq[i]), self.reactivities[i]) # positive example\n\n# train_dataset = BedPeaksDataset(seqs, reactivities)\n# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers = 0)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:32.299423Z",
          "iopub.status.idle": "2023-11-16T21:49:35.158833Z",
          "iopub.execute_input": "2023-11-16T21:49:32.299779Z",
          "shell.execute_reply": "2023-11-16T21:49:35.157446Z",
          "shell.execute_reply.started": "2023-11-16T21:49:32.299749Z"
        },
        "cell_id": "65306f0074a44605bb2a4b6c26ce4459",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "2f0855e35c20470993004c1e86e401a4"
    },
    {
      "cell_type": "code",
      "source": "class CNN_1d(nn.Module):\n\n    def __init__(self,\n                 n_output_channels = 206,\n                 filter_widths = [15, 5],\n                 num_chunks = 5,\n                 max_pool_factor = 4,\n                 nchannels = [5, 32, 32],\n                 n_hidden = 32,\n                 dropout = 0.2):\n\n        super(CNN_1d, self).__init__()\n        self.rf = 0 # running estimate of the receptive field\n        self.chunk_size = 1 # running estimate of num basepairs corresponding to one position after convolutions\n\n        conv_layers = []\n        for i in range(len(nchannels)-1):\n            conv_layers += [ nn.Conv1d(nchannels[i], nchannels[i+1], filter_widths[i], padding = 0),\n                        nn.BatchNorm1d(nchannels[i+1]), # tends to help give faster convergence: https://arxiv.org/abs/1502.03167\n                        nn.Dropout2d(dropout), # popular form of regularization: https://jmlr.org/papers/v15/srivastava14a.html\n                        nn.MaxPool1d(max_pool_factor),\n                        nn.ELU(inplace=True)  ] # popular alternative to ReLU: https://arxiv.org/abs/1511.07289\n            assert(filter_widths[i] % 2 == 1) # assume this\n            self.rf += (filter_widths[i] - 1) * self.chunk_size\n            self.chunk_size *= max_pool_factor\n\n        # If you have a model with lots of layers, you can create a list first and\n        # then use the * operator to expand the list into positional arguments, like this:\n        self.conv_net = nn.Sequential(*conv_layers)\n\n        # Calculate the output size after convolutions and pooling\n        total_length = 206  # Assuming the width of the input matrix is 206\n        for filter_width in filter_widths:\n            total_length = (total_length - filter_width) // max_pool_factor + 1\n\n        # Calculate the correct number of features to pass to the first linear layer\n        conv_output_features = nchannels[-1] * total_length\n        self.dense_net = nn.Sequential(nn.Linear(conv_output_features, n_hidden),\n                                        nn.Dropout(dropout),\n                                        nn.ELU(inplace=True),\n                                        nn.Linear(n_hidden, n_output_channels))\n        \n#         self.conv_net = nn.Sequential(*conv_layers)\n\n#         self.seq_len = num_chunks * self.chunk_size + self.rf # amount of sequence context required\n\n#         print(\"Receptive field:\", self.rf, \"Chunk size:\", self.chunk_size, \"Number chunks:\", num_chunks)\n\n#         self.dense_net = nn.Sequential( nn.Linear(nchannels[-1] * num_chunks, n_hidden),\n#                                         nn.Dropout(dropout),\n#                                         nn.ELU(inplace=True),\n#                                         nn.Linear(n_hidden, n_output_channels) )\n\n    def forward(self, x):\n        net = self.conv_net(x)\n        net = net.view(net.size(0), -1)\n        net = self.dense_net(net)\n        return(net)\n",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:35.160452Z",
          "iopub.status.idle": "2023-11-16T21:49:35.175322Z",
          "iopub.execute_input": "2023-11-16T21:49:35.161289Z",
          "shell.execute_reply": "2023-11-16T21:49:35.173855Z",
          "shell.execute_reply.started": "2023-11-16T21:49:35.161254Z"
        },
        "cell_id": "d05e791c70f848d481d28bd7325f5a6f",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "94d0262306e544cd8853b2e8768c962b"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "cell_id": "7df3bd9f290a4de990b371fbe8da5c90",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "624c6cd2f3034d83996e19bbc2894e01"
    },
    {
      "cell_type": "code",
      "source": "def run_one_epoch(train_flag, dataloader, cnn_1d, optimizer, device=\"cuda\"):\n\n    torch.set_grad_enabled(train_flag)\n    cnn_1d.train() if train_flag else cnn_1d.eval()\n\n    losses = []\n    accuracies = []\n\n    for (x,y) in dataloader: # collection of tuples with iterator\n        x = x.float()\n        y = y.float()\n        (x, y) = ( x.to(device), y.to(device) ) # transfer data to GPU\n\n        output = cnn_1d(x) # forward pass\n        output = output.squeeze() # remove spurious channel dimension\n        loss = F.mse_loss(output, y).float()\n\n        if train_flag:\n            loss.backward() # back propagation\n            optimizer.step()\n            optimizer.zero_grad()\n\n        losses.append(loss.detach().cpu().numpy())\n        # accuracy = torch.mean( ( (output > 0.0) == (y > 0.5) ).float() ) # output is in logit space so threshold is 0.\n        # accuracies.append(accuracy.detach().cpu().numpy())\n\n    return( np.mean(losses))",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:35.176817Z",
          "iopub.status.idle": "2023-11-16T21:49:35.190874Z",
          "iopub.execute_input": "2023-11-16T21:49:35.177111Z",
          "shell.execute_reply": "2023-11-16T21:49:35.190058Z",
          "shell.execute_reply.started": "2023-11-16T21:49:35.177086Z"
        },
        "cell_id": "46f527d7136d438c987d3272b8ea2943",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "869c1e46298940669dc5384d210e9dc9"
    },
    {
      "cell_type": "code",
      "source": "# F.mse_loss(torch.tensor([5., 5., 5.]), torch.tensor([6., 6., 6.]))",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:35.194063Z",
          "iopub.status.idle": "2023-11-16T21:49:35.207605Z",
          "iopub.execute_input": "2023-11-16T21:49:35.195231Z",
          "shell.execute_reply": "2023-11-16T21:49:35.205655Z",
          "shell.execute_reply.started": "2023-11-16T21:49:35.195203Z"
        },
        "cell_id": "73c86ef67578476cb7d2eb29a51888c1",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "b158f08a20a44150aaff2fdfdabfa5ad"
    },
    {
      "cell_type": "code",
      "source": "def train_model(cnn_1d, train_dataloader, test_dataloader, epochs=100, patience=10, verbose = True, lr = 0.001, weight_decay = 0):\n    \"\"\"\n    Train a 1D CNN model and record accuracy metrics.\n    \"\"\"\n    # Move the model to the GPU here to make it runs there, and set \"device\" as above\n    # TODO CODE\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    cnn_1d.to(device)\n\n    # 1. Make new BedPeakDataset and DataLoader objects for both training and validation data.\n    # TODO CODE\n#     train_dataset = BedPeaksDataset(train_data, genome, cnn_1d.seq_len)\n#     train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, num_workers = 0)\n    # validation_dataset = BedPeaksDataset(validation_data, genome, cnn_1d.seq_len)\n    # validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000)\n\n    # 2. Instantiates an optimizer for the model.\n    # TODO CODE\n    optimizer = torch.optim.Adam(cnn_1d.parameters(), amsgrad=True, lr = lr, weight_decay = weight_decay)\n\n    # 3. Run the training loop with early stopping.\n    # TODO CODE\n    train_losses = []\n    test_losses = []\n    # patience_counter = patience\n    best_test_loss = np.inf\n    check_point_filename = 'cnn_1d_checkpoint.pt' # to save the best model fit to date\n    for epoch in range(epochs):\n        start_time = timeit.default_timer()\n        train_loss = run_one_epoch(True, train_dataloader, cnn_1d, optimizer, device)\n        test_loss = run_one_epoch(False, test_dataloader, cnn_1d, optimizer, device)\n        train_losses.append(train_loss)\n        test_losses.append(test_loss)\n        # train_accs.append(train_acc)\n        # val_accs.append(val_acc)\n        if test_loss < best_test_loss:\n            torch.save(cnn_1d.state_dict(), check_point_filename)\n            best_test_loss = test_loss\n            patience_counter = patience\n        else:\n            patience_counter -= 1\n            if patience_counter <= 0:\n                cnn_1d.load_state_dict(torch.load(check_point_filename)) # recover the best model so far\n                break\n        elapsed = float(timeit.default_timer() - start_time)\n        print(\"Epoch {} took {:.2f}s. Train loss: {:.4f}., Test loss: {:.4f}. Pariance: {}\".format(epoch+1, elapsed, train_loss, test_loss, patience))\n\n    # 4. Return the fitted model (not strictly necessary since this happens \"in place\"), train and validation accuracies.\n    # TODO CODE\n    return(cnn_1d, train_looses, test_losses)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:35.210075Z",
          "iopub.status.idle": "2023-11-16T21:49:35.221372Z",
          "iopub.execute_input": "2023-11-16T21:49:35.210417Z",
          "shell.execute_reply": "2023-11-16T21:49:35.219905Z",
          "shell.execute_reply.started": "2023-11-16T21:49:35.210389Z"
        },
        "cell_id": "961ca3355ceb44409ea99245be8bc1ed",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "68de88e8a13d4b3dac8ce6691415f035"
    },
    {
      "cell_type": "code",
      "source": "pip install dask_ml",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:35.222471Z",
          "iopub.status.idle": "2023-11-16T21:49:47.382748Z",
          "iopub.execute_input": "2023-11-16T21:49:35.222934Z",
          "shell.execute_reply": "2023-11-16T21:49:47.381471Z",
          "shell.execute_reply.started": "2023-11-16T21:49:35.222908Z"
        },
        "cell_id": "1da85da51f0f46eda2f1b54a4c4b1ebd",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "b134134c7cf84f109c2bfb7dd399e734"
    },
    {
      "cell_type": "code",
      "source": "# apply SN-filter\nfrom dask_ml.model_selection import train_test_split\ndf_sn = ddf[ddf[\"SN_filter\"]==1]\n\n# split into 2A3 MaP and DMS MaP datasets\ndf_2A3 = df_sn[df_sn[\"experiment_type\"]==\"2A3_MaP\"]\ndf_DMS = df_sn[df_sn[\"experiment_type\"]==\"DMS_MaP\"]\n\n# split into train and test\nX_2A3 = df_2A3[\"sequence\"]\ny_2A3 = df_2A3.loc[:, df_2A3.columns.str.fullmatch(\"reactivity_\\d\\d\\d\\d\")]\nX_2A3_train, X_2A3_test, y_2A3_train, y_2A3_test = train_test_split(X_2A3, y_2A3, test_size=0.2, shuffle=True, blockwise=True, random_state=42)\n\nX_DMS = df_DMS[\"sequence\"]\ny_DMS = df_DMS.loc[:, df_DMS.columns.str.fullmatch(\"reactivity_\\d\\d\\d\\d\")]\nX_DMS_train, X_DMS_test, y_DMS_train, y_DMS_test = train_test_split(X_DMS, y_DMS, test_size=0.2, shuffle=True, blockwise=True, random_state=42)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:47.386009Z",
          "iopub.status.idle": "2023-11-16T21:49:48.503163Z",
          "iopub.execute_input": "2023-11-16T21:49:47.386354Z",
          "shell.execute_reply": "2023-11-16T21:49:48.501669Z",
          "shell.execute_reply.started": "2023-11-16T21:49:47.386317Z"
        },
        "cell_id": "200af40ed5eb404ca253ea13d4712dab",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "d0f82b036cf34726b4ec20b271dc45ec"
    },
    {
      "cell_type": "code",
      "source": "def df_toArray(ddf1, ddf2):\n    subset_columns = []\n    for i in range(206):\n        subset_columns.append(\"reactivity_0\"+str(i+1).zfill(3))\n\n    # Use .to_dask_array() to convert the subset of the DataFrame to a Dask Array\n    # subset = small_set[subset_columns]\n\n    # Compute the subset of the Dask DataFrame and convert it to a Pandas DataFrame\n    reactivities = ddf2.compute().to_numpy()\n\n    row_means = np.nanmean(reactivities, axis=1)\n\n    # Iterate over each element and replace NaN with the row mean\n    for i, row in enumerate(reactivities):\n        mask = np.isnan(row)\n        reactivities[i, mask] = row_means[i]\n\n    reactivities\n\n    seqs = ddf1.compute().tolist()\n    return seqs, reactivities\n    \n# df_toArray(X_2A3_train, y_2A3_train)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:49:48.504423Z",
          "iopub.status.idle": "2023-11-16T21:49:48.512642Z",
          "iopub.execute_input": "2023-11-16T21:49:48.50473Z",
          "shell.execute_reply": "2023-11-16T21:49:48.511063Z",
          "shell.execute_reply.started": "2023-11-16T21:49:48.504705Z"
        },
        "cell_id": "8a21c2be32184d64b44f6b2d5186c3ca",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "bfd4428ce53c417b8570e7f929b8d1b7"
    },
    {
      "cell_type": "code",
      "source": "seqs, reactivities = df_toArray(X_2A3_train, y_2A3_train)\ntrain_dataset = BedPeaksDataset(seqs, reactivities)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n\nseqs, reactivities = df_toArray(X_2A3_test, y_2A3_test)\ntest_dataset = BedPeaksDataset(seqs, reactivities)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1000, num_workers = 0)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T22:04:13.574296Z",
          "iopub.status.idle": "2023-11-16T22:07:23.990266Z",
          "iopub.execute_input": "2023-11-16T22:04:13.574963Z",
          "shell.execute_reply": "2023-11-16T22:07:23.988528Z",
          "shell.execute_reply.started": "2023-11-16T22:04:13.574911Z"
        },
        "cell_id": "cbd97a2d6337418d873f8be0bf17c953",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "758723d4b1f74f99a9a9009cf1f47035"
    },
    {
      "cell_type": "code",
      "source": "import timeit\nmy_cnn1d_1 = CNN_1d()\n# print(my_cnn1d.seq_le\nmy_cnn1d_1 = my_cnn1d_1.float()\nmy_cnn1d_1, train_losses, test_losses = train_model(my_cnn1d_1, train_dataloader, test_dataloader, lr = 0.001, weight_decay = 0)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T22:07:42.162504Z",
          "iopub.status.idle": "2023-11-16T22:11:55.929232Z",
          "iopub.execute_input": "2023-11-16T22:07:42.162924Z",
          "shell.execute_reply": "2023-11-16T22:11:55.927143Z",
          "shell.execute_reply.started": "2023-11-16T22:07:42.162892Z"
        },
        "cell_id": "ca253ed8477a4bf7bc979c074189406a",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "5665b15b64c04b3a807571a886366bef"
    },
    {
      "cell_type": "code",
      "source": "device = 'cuda'\noutputs = []\nexpected = []\nfor (x,y) in train_dataloader: # iterate over batches\n    # print(y)\n    if torch.cuda.is_available():\n        x = x.to(device)\n    output = my_cnn1d_1(x).squeeze() # your awesome model here!\n    output = torch.sigmoid(output)\n    output_np = output.detach().cpu().numpy()\n    outputs.append(output_np)\n    expected.append(y.numpy())\noutput_np = np.concatenate(outputs)\nexpected_np = np.concatenate(expected)\n\nprint(output_np)\nprint(expected_np)\n\noutput_np = output_np.clip(0, 1)\nexpected_np = expected_np.clip(0,1)\n\nmae1 = np.mean(np.abs(output_np - expected_np))\nmae1",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:47.902265Z",
          "iopub.status.idle": "2023-11-16T21:55:52.801695Z",
          "iopub.execute_input": "2023-11-16T21:55:47.902606Z",
          "shell.execute_reply": "2023-11-16T21:55:52.800228Z",
          "shell.execute_reply.started": "2023-11-16T21:55:47.902585Z"
        },
        "cell_id": "8a155b0c433e4ab58b8f7d2b652b39bd",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "c6e11fd5718d4999bd046b94de4a1794"
    },
    {
      "cell_type": "code",
      "source": "seqs, reactivities = df_toArray(X_DMS_train, y_DMS_train)\ntrain_dataset = BedPeaksDataset(seqs, reactivities)\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n\nseqs, reactivities = df_toArray(X_DMS_test, y_DMS_train)\ntest_dataset = BedPeaksDataset(seqs, reactivities)\ntest_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n\nmy_cnn1d_2 = CNN_1d()\n# print(my_cnn1d.seq_le\nmy_cnn1d_2 = my_cnn1d_2.float()\nmy_cnn1d_2, train_losses, test_losses = train_model(my_cnn1d_2, train_dataloader, test_dataloader, lr = 0.001, weight_decay = 0)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T22:12:12.467827Z",
          "iopub.status.idle": "2023-11-16T22:22:11.959762Z",
          "iopub.execute_input": "2023-11-16T22:12:12.468281Z",
          "shell.execute_reply": "2023-11-16T22:22:11.957568Z",
          "shell.execute_reply.started": "2023-11-16T22:12:12.468252Z"
        },
        "cell_id": "265f4ce88d7e4005824ab638af1b52b2",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "729e91642b04447289faaf4747d71b4c"
    },
    {
      "cell_type": "code",
      "source": "device = 'cuda'\noutputs = []\nexpected = []\nfor (x,y) in train_dataloader: # iterate over batches\n    # print(y)\n    if torch.cuda.is_available():\n        x = x.to(device)\n    output = my_cnn1d_2(x).squeeze() # your awesome model here!\n    output = torch.sigmoid(output)\n    output_np = output.detach().cpu().numpy()\n    outputs.append(output_np)\n    expected.append(y.numpy())\noutput_np = np.concatenate(outputs)\nexpected_np = np.concatenate(expected)\n\nprint(output_np)\nprint(expected_np)\n\noutput_np = output_np.clip(0, 1)\nexpected_np = expected_np.clip(0,1)\n\nmae2 = np.mean(np.abs(output_np - expected_np))\nmae2",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:40.820982Z",
          "iopub.status.idle": "2023-11-16T21:55:40.821422Z",
          "shell.execute_reply": "2023-11-16T21:55:40.821217Z",
          "shell.execute_reply.started": "2023-11-16T21:55:40.821197Z"
        },
        "cell_id": "70cfd26a3efe4a0980babb3881a16926",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "2233812fbbac4712ba3740b68173e461"
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true,
        "cell_id": "fa53be4387a14de5a6490074cd6bf169",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "1b621cc5a4a24e87b71d25c8abf0c367"
    },
    {
      "cell_type": "code",
      "source": "# def train_model(cnn_1d, train_data, validation_data, epochs=100, patience=10, verbose = True):\n#     \"\"\"\n#     Train a 1D CNN model and record accuracy metrics.\n#     \"\"\"\n#     # Move the model to the GPU here to make it runs there, and set \"device\" as above\n#     # TODO CODE\n#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#     cnn_1d.to(device)\n\n#     # 1. Make new BedPeakDataset and DataLoader objects for both training and validation data.\n#     # TODO CODE\n#     train_dataset = BedPeaksDataset(train_data, genome, cnn_1d.seq_len)\n#     train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=1000, num_workers = 0)\n#     validation_dataset = BedPeaksDataset(validation_data, genome, cnn_1d.seq_len)\n#     validation_dataloader = torch.utils.data.DataLoader(validation_dataset, batch_size=1000)\n\n#     # 2. Instantiates an optimizer for the model.\n#     # TODO CODE\n#     optimizer = torch.optim.Adam(cnn_1d.parameters(), amsgrad=True)\n\n#     # 3. Run the training loop with early stopping.\n#     # TODO CODE\n#     train_accs = []\n#     val_accs = []\n#     patience_counter = patience\n#     best_val_loss = np.inf\n#     check_point_filename = 'cnn_1d_checkpoint.pt' # to save the best model fit to date\n#     for epoch in range(epochs):\n#         start_time = timeit.default_timer()\n#         train_loss, train_acc = run_one_epoch(True, train_dataloader, cnn_1d, optimizer, device)\n#         val_loss, val_acc = run_one_epoch(False, validation_dataloader, cnn_1d, optimizer, device)\n#         train_accs.append(train_acc)\n#         val_accs.append(val_acc)\n#         if val_loss < best_val_loss:\n#             torch.save(cnn_1d.state_dict(), check_point_filename)\n#             best_val_loss = val_loss\n#             patience_counter = patience\n#         else:\n#             patience_counter -= 1\n#             if patience_counter <= 0:\n#                 cnn_1d.load_state_dict(torch.load(check_point_filename)) # recover the best model so far\n#                 break\n#         elapsed = float(timeit.default_timer() - start_time)\n#         print(\"Epoch %i took %.2fs. Train loss: %.4f acc: %.4f. Val loss: %.4f acc: %.4f. Patience left: %i\" %\n#               (epoch+1, elapsed, train_loss, train_acc, val_loss, val_acc, patience_counter ))\n\n#     # 4. Return the fitted model (not strictly necessary since this happens \"in place\"), train and validation accuracies.\n#     # TODO CODE\n#     return(cnn_1d, train_accs, val_accs)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:40.822852Z",
          "iopub.status.idle": "2023-11-16T21:55:40.823274Z",
          "shell.execute_reply": "2023-11-16T21:55:40.823087Z",
          "shell.execute_reply.started": "2023-11-16T21:55:40.823068Z"
        },
        "cell_id": "085b22374db646d78fb0194df8dbc87a",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "91cddd3bf95f4f89bc3a597c97d00870"
    },
    {
      "cell_type": "code",
      "source": "first_10_rows_ddf = dms_ddf.head(10)\nseqs1 = dms_ddf['sequence'].compute().tolist()\nseqs2 = twoa3_ddf['sequence'].compute().tolist()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:40.824421Z",
          "iopub.status.idle": "2023-11-16T21:55:40.824815Z",
          "shell.execute_reply": "2023-11-16T21:55:40.824632Z",
          "shell.execute_reply.started": "2023-11-16T21:55:40.824613Z"
        },
        "cell_id": "a9c9e588fb0849d9b69149a5903ecd65",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "2f311b11adcd43f8b1a3b3f3c97b16cb"
    },
    {
      "cell_type": "code",
      "source": "for i in range(len(seqs1)):\n    if len(seqs1[i])!= 170 or len(seqs1[i])!= 177:\n        print(len(seqs1[i]))",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:40.826152Z",
          "iopub.status.idle": "2023-11-16T21:55:40.826578Z",
          "shell.execute_reply": "2023-11-16T21:55:40.826396Z",
          "shell.execute_reply.started": "2023-11-16T21:55:40.826365Z"
        },
        "cell_id": "0798a931d82648f4b4d991194fbd87aa",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "1ae56670ce5145e1bbbaf84576af1010"
    },
    {
      "cell_type": "markdown",
      "source": "The unigram, bigram, etc. may have different reactivity distributions for their nucleotides.\n\nLet's examine the reactivity distributions for the nucleotides of unigram, bigram, etc.",
      "metadata": {
        "cell_id": "98ac4a1de51f4472ade23c9d55bf159f",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "f2a4e96a46744088beed5aee8f3e1632"
    },
    {
      "cell_type": "code",
      "source": "# Given a row with a sequence and a subsequence,\n# returns a list of the reactivities of all\n# instances of the subsequence in the sequence\n# Example:\n#    getReactOfSubseqForRow(row, 'AUG') ->  [[-0.027, 0.429, 0.817], [-0.079, -0.014, 0.328]]\ndef getReactOfSubseqForRow(row, subseq):\n    \n    react = []\n    \n    for i in range(len(row['sequence']) - len(subseq)):\n        \n        if row['sequence'][i:i+len(subseq)] == subseq:\n            \n            react_curr = []\n            for j in range(i,i+len(subseq)):\n                react_curr.append(row['reactivity_' + f'{j+1:04}'])\n            \n            if not np.isnan(react_curr).any():\n                react.append(react_curr)\n    \n    return np.array(react)\n\n# Given a dataframe with sequences and a subsequence,\n# returns a dataframe of the reactivities of all\n# instances of the subsequence in the sequence\n# Example:\n#    getReactOfSubseqForRow(df, 'AUG') ->  \n#        ###############################\n#        # react_A # react_U # react_G #\n#        ###############################\n#        #   1.221 #   1.031 #   1.098 #\n#        #   1.272 #   1.453 #   1.007 #\n#        #   4.446 #   0.000 #   0.000 #\n#        ###############################\ndef getReactOfSubseqForFrame(df, subseq):\n    return df.apply(\n                getReactOfSubseqForRow,\n                subseq=subseq,\n                axis=1\n            ).explode().dropna().to_frame().apply(\n                lambda row: row[0],\n                axis=1,\n                result_type='expand'\n            ).rename(columns = {\n                i: f'react_{val}_{i}'\n                for i, val in enumerate(subseq)\n            })",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:40.82786Z",
          "iopub.status.idle": "2023-11-16T21:55:40.82828Z",
          "shell.execute_reply": "2023-11-16T21:55:40.828107Z",
          "shell.execute_reply.started": "2023-11-16T21:55:40.828087Z"
        },
        "cell_id": "5907494a312d4e348e867e995a97dbd9",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "8c12ffb9cadb4a41bd6954cf716f8241"
    },
    {
      "cell_type": "code",
      "source": "pdf_sn1 = shfl_ddf[shfl_ddf[\"SN_filter\"] == 1]\npdf_2a3 = pdf_sn1[pdf_sn1[\"experiment_type\"] == \"2A3_MaP\"].head(n=1000, compute=True)",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:40.829815Z",
          "iopub.status.idle": "2023-11-16T21:55:40.830244Z",
          "shell.execute_reply": "2023-11-16T21:55:40.830062Z",
          "shell.execute_reply.started": "2023-11-16T21:55:40.830043Z"
        },
        "cell_id": "ce78f929749c46c1b676bcb6ed5530b5",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "2788caa9bc004e4b84ed5789ae8fdb30"
    },
    {
      "cell_type": "code",
      "source": "for i in range(1, 3):\n    for subseq in product(\"AUGC\", repeat = i):\n        getReactOfSubseqForFrame(pdf_2a3, \"\".join(subseq)).hist(bins=20)\n        plt.show()\n        plt.close()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:40.831447Z",
          "iopub.status.idle": "2023-11-16T21:55:40.831841Z",
          "shell.execute_reply": "2023-11-16T21:55:40.831669Z",
          "shell.execute_reply.started": "2023-11-16T21:55:40.831651Z"
        },
        "cell_id": "e886c089481f409e81e4ee30018048bd",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "ca5aca2fb45347fd8a654b393aac32dc"
    },
    {
      "cell_type": "markdown",
      "source": "The histograms tend to be right skewed and peak close to 0.",
      "metadata": {
        "cell_id": "5103f1525d014feab81e4ad84a7d950d",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "97ce891dc5db43d8b0fbb51598bc0248"
    },
    {
      "cell_type": "markdown",
      "source": "In the competition, the goal is not to predict reactivity, but reactivity (bounded), which is reactivity bounded between 0 and 1.\n\nLet's examine reactivity bounded.",
      "metadata": {
        "cell_id": "fa3381f9585c4b0ebea355da64a92a07",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "8fe04bc1110d431082fb43e32560c967"
    },
    {
      "cell_type": "code",
      "source": "for i in range(1, 3):\n    for subseq in product(\"AUGC\", repeat = i):\n        getReactOfSubseqForFrame(pdf_2a3, \"\".join(subseq)).clip(0,1).hist(bins=20)\n        plt.show()\n        plt.close()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:40.833163Z",
          "iopub.status.idle": "2023-11-16T21:55:40.833546Z",
          "shell.execute_reply": "2023-11-16T21:55:40.833366Z",
          "shell.execute_reply.started": "2023-11-16T21:55:40.833347Z"
        },
        "cell_id": "7085846f31c84f67b3a974077ceba329",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "32e1e22c3616484e82b2918bbcb47084"
    },
    {
      "cell_type": "markdown",
      "source": "The histograms often have 2 peaks, one at 0 and one at 1.",
      "metadata": {
        "cell_id": "45f7062e016f4473b710b3aa03553a29",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "ab1fe1c83a144abaacaeb2a7c4bd31d3"
    },
    {
      "cell_type": "markdown",
      "source": "The histograms seem to be very similar across subsequences.\n\nLet's examine the distribution for subsequences of length 1. I am using a tool from [here](https://medium.com/the-researchers-guide/finding-the-best-distribution-that-fits-your-data-using-pythons-fitter-library-319a5a0972e9).",
      "metadata": {
        "cell_id": "2f641fe6c2964fe2b820b21a9ae82e63",
        "deepnote_cell_type": "markdown"
      },
      "block_group": "6263bd58a0ce477db30b659b25edd424"
    },
    {
      "cell_type": "code",
      "source": "for b in \"AUGC\":\n    react = getReactOfSubseqForFrame(pdf_2a3, b).iloc[:, 0].values\n    f = Fitter(react)\n    f.fit(progress=True)\n    print(f.summary())\n    print(b)\n    plt.show()",
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2023-11-16T21:55:40.834787Z",
          "iopub.status.idle": "2023-11-16T21:55:40.835226Z",
          "shell.execute_reply": "2023-11-16T21:55:40.835019Z",
          "shell.execute_reply.started": "2023-11-16T21:55:40.83498Z"
        },
        "cell_id": "85e9124b90d34670b524a08434a2cce1",
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": null,
      "block_group": "6f752561ca6245068a25f4fd891df1aa"
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=fe343e39-d2c0-4296-915d-091d9a42752d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kaggle": {
      "language": "python",
      "sourceType": "notebook",
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 51294,
          "sourceType": "competition",
          "databundleVersionId": 6923401
        }
      ],
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "dockerImageVersionId": 30558
    },
    "deepnote": {},
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "nbconvert_exporter": "python"
    },
    "deepnote_notebook_id": "e93f1734864c4c24af3643a872cbcbaf",
    "deepnote_execution_queue": []
  }
}